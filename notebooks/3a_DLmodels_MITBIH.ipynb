{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-21T08:21:29.078990Z","iopub.status.busy":"2024-03-21T08:21:29.078141Z","iopub.status.idle":"2024-03-21T08:21:29.100221Z","shell.execute_reply":"2024-03-21T08:21:29.099372Z","shell.execute_reply.started":"2024-03-21T08:21:29.078956Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra \n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.metrics import accuracy_score, classification_report\n","import torch\n","import matplotlib.pyplot as plt\n","import os"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["All Datasets are already available.\n"]}],"source":["#In this cell, the datasets are downloaded via the KaggleAPI directly from the source. It might be necessary to authentificate first via Webbrowser to make this work.\n","#FUrthermore, a folder ../data is created, which is on the .gitignore list. In this folder, large files >100mb and the original datasets MITBIH and PTBDB are stored.\n","\n","from kaggle.api.kaggle_api_extended import KaggleApi\n","\n","#configuring and authentification with kaggle api. This could be configured so that a authentification mask is shown?\n","api = KaggleApi()\n","api.authenticate()\n","\n","#Configuring the metadata for the ecg heartbeat data (original username etc)\n","dataset_owner = \"shayanfazeli\"\n","dataset_name = \"heartbeat\"\n","\n","#Configuring a download path that is NOT in the current github repo (so the big files are not pushed and cause an error!) --> Links to filepaths have to be dynamically adjusted\n","download_path = \"../data/KAGGLE_datasets\" #In this case we use the data folder that is in the .gitignore list and therefore not pushed! To keep everything in one local repo.\n","\n","# Download structure: First check if dataset is already downloaded, else download it and store it in download path (should be outside git repo!)\n","dataset_folder = os.path.join(download_path, dataset_name)\n","if not os.path.exists(dataset_folder):\n","    # Case 1: Dataset path is not created --> Create it and download datasets into it\n","    api.dataset_download_files(dataset_owner + \"/\" + dataset_name, path=download_path + \"/\" + dataset_name, unzip=True)\n","    print(\"Datasets are downloaded and unzipped.\")\n","else:\n","    # Case 2: Folder is created, but datasets might be missing\n","    missing_files = [] \n","    for file_name in [\"mitbih_test.csv\", \"mitbih_train.csv\", \"ptbdb_abnormal.csv\", \"ptbdb_normal.csv\"]:  # These are the hardcoded names of the datasets that should be downloaded.\n","        file_path = os.path.join(dataset_folder, file_name)\n","        if not os.path.exists(file_path):\n","            missing_files.append(file_name)\n","\n","    if missing_files:\n","        # If the list contains missing files, download ALL files and overwrite the old folder.\n","        api.dataset_download_files(dataset_owner + \"/\" + dataset_name, path=download_path + \"/\" + dataset_name, unzip=True, force=True)\n","        print(\"Missing data was donwloaded and unzipped. All Datasets are now available.\")\n","    else:\n","        print(\"All Datasets are already available.\")\n","\n","#Creating new variable that links to the datasets and can be used in the rest of the code.\n","path_to_datasets = download_path + \"/\" + dataset_name "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:29.102011Z","iopub.status.busy":"2024-03-21T08:21:29.101733Z","iopub.status.idle":"2024-03-21T08:21:29.106371Z","shell.execute_reply":"2024-03-21T08:21:29.105285Z","shell.execute_reply.started":"2024-03-21T08:21:29.101988Z"},"trusted":true},"outputs":[],"source":["np.set_printoptions(precision=4)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:29.107643Z","iopub.status.busy":"2024-03-21T08:21:29.107375Z","iopub.status.idle":"2024-03-21T08:21:41.062102Z","shell.execute_reply":"2024-03-21T08:21:41.060979Z","shell.execute_reply.started":"2024-03-21T08:21:29.107621Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataframes MITBIH correctly read into workspace\n"]}],"source":["# This cell now makes use of the downloadfolder for the datasets. If already available locally, the filepaths can be changed.\n","df_train= pd.read_csv(path_to_datasets + \"/\" + 'mitbih_train.csv', header=None)\n","df_test=pd.read_csv(path_to_datasets + \"/\" +  'mitbih_test.csv',header=None)\n","print(\"Dataframes MITBIH correctly read into workspace\")\n","\n","df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","#split target and value\n","train_target=df_train[187]\n","test_target=df_test[187]\n","train=df_train.drop(187,axis=1)\n","test=df_test.drop(187,axis=1)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.065807Z","iopub.status.busy":"2024-03-21T08:21:41.065436Z","iopub.status.idle":"2024-03-21T08:21:41.072551Z","shell.execute_reply":"2024-03-21T08:21:41.071545Z","shell.execute_reply.started":"2024-03-21T08:21:41.065770Z"},"trusted":true},"outputs":[],"source":["#Switches to decide the dataset sampling method and which models should be run\n","class Config_Sampling:\n","    oversample = False #equals to B_SMOTE\n","    undersample = False\n","    sample_name = \"UNDEFINED_SAMPLE\"\n","    \n","Train_Simple_ANN = True #Trains the simple ANN\n","Train_Simple_CNN = True #Trains the simple CNN\n","Train_Advanced_CNN = True #Trains the advanced CNN\n"," "]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.074675Z","iopub.status.busy":"2024-03-21T08:21:41.074106Z","iopub.status.idle":"2024-03-21T08:21:41.088102Z","shell.execute_reply":"2024-03-21T08:21:41.087129Z","shell.execute_reply.started":"2024-03-21T08:21:41.074641Z"},"trusted":true},"outputs":[],"source":["oversampler = SMOTE()\n","undersampler = RandomUnderSampler()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.089884Z","iopub.status.busy":"2024-03-21T08:21:41.089546Z","iopub.status.idle":"2024-03-21T08:21:41.099952Z","shell.execute_reply":"2024-03-21T08:21:41.099018Z","shell.execute_reply.started":"2024-03-21T08:21:41.089853Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using the original mitbih dataset\n","Sample Name: MITBIH_A_Original\n"]}],"source":["#Based on the configuration in the Config_Sampling Class, the datasets are sampled and the sample name is modified accordingly\n","if Config_Sampling.oversample:\n","    train, train_target = oversampler.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n","    Config_Sampling.sample_name = \"MITBIH_B_SMOTE\"\n","    print(\"Sample Name:\", Config_Sampling.sample_name)\n","elif Config_Sampling.undersample:\n","    train, train_target = undersampler.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n","    Config_Sampling.sample_name = \"MITBIH_C_RUS\"\n","    print(\"Sample Name:\", Config_Sampling.sample_name)\n","else: \n","    print(\"Using the original mitbih dataset\")\n","    Config_Sampling.sample_name = \"MITBIH_A_Original\"\n","    print(\"Sample Name:\", Config_Sampling.sample_name)"]},{"cell_type":"markdown","metadata":{},"source":["## **Simple Artificial Neural Network**\n","ANN without convolutional layers. Only Dense layers are used. No Pooling, Flattening or Dropping out. Base model for later comparison."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"markdown","metadata":{},"source":["Implement Torch Dataset object"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["class ECG_Dataset(Dataset):\n","    def __init__(self, csv_file, transform=None, target_transform=None):\n","        self.dataframe = csv_file.values\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","        #return self.dataframe.shape[0] # Alternative notation\n","\n","    def __getitem__(self, idx):\n","        inputs = torch.tensor(self.dataframe[idx,:-1]).to(torch.float32)\n","        label = torch.tensor(self.dataframe[idx,-1]).long()\n","\n","        return inputs, label"]},{"cell_type":"markdown","metadata":{},"source":["Custom function for preprocessing (to elaborate later, currently just returns the input itself)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["class Lambda(nn.Module):\n","    def __init__(self, func):\n","        super().__init__()\n","        self.func = func\n","\n","    def forward(self, x):\n","        return self.func(x)\n","\n","\n","def preprocess(x):\n","    return x * torch.Tensor([1.0])"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["# Define the ANN model\n","class SimpleANN(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super().__init__()\n","        self.fc0 = nn.Sequential(Lambda(preprocess))\n","        self.fc1 = nn.Linear(input_size, 212)\n","        self.fc2 = nn.Linear(212, 150)  \n","        self.fc3 = nn.Linear(150, 60) \n","        self.fc4 = nn.Linear(60, 24)  \n","        self.fc5 = nn.Linear(24, 12)  # Hidden to output layer\n","        self.fc6 = nn.Linear(12, output_size)  # Hidden to output layer\n","        self.relu = nn.LeakyReLU(negative_slope=0.001)    # Activation function\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.sigmoid = nn.Sigmoid() \n","\n","    def forward(self, x):\n","        x = self.fc0(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.fc3(x)\n","        x = self.relu(x)\n","        x = self.fc4(x)\n","        x = self.relu(x)\n","        x = self.fc5(x)\n","        x = self.relu(x)\n","        x = self.fc6(x)\n","        return x"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def batch_loss_train(outputs, labels, loss_fn, optimizer):\n","    loss = loss_fn(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","    \n","    return loss.item()"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["def batch_loss_test(outputs, labels, loss_fn):\n","    loss = loss_fn(outputs, labels)    \n","    return loss.item()"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def test_loop(dataloader, model, loss_fn):\n","    # Set the model to evaluation mode - important for batch normalization and dropout layers\n","    model.eval()\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    test_loss, correct = 0, 0\n","\n","    # Evaluating the model with torch.no_grad()    \n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            test_loss += loss_fn(pred, y) / len(X)\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","    #test_loss /= num_batches\n","    correct /= size\n","    print(f\"Test set => Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["def train_loop(dataloader, model, loss_fn, optimizer):\n","    model.train()\n","    train_loss = 0.0\n","    \n","    for inputs, labels in dataloader:\n","        \n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        train_loss += batch_loss_train(outputs,labels,loss_fn, optimizer) / len(inputs)\n","\n","    print(f'Train loss: {train_loss}')\n","    "]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["def get_data(train_ds, valid_ds, bs):\n","    return (\n","        DataLoader(train_ds, batch_size=bs, shuffle=True),\n","        DataLoader(valid_ds, batch_size=bs),\n","    )"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n","    for t in range(epochs):  \n","        print(f\"Epoch {t+1}   -------------------------------\")\n","        train_loop(train_dl, model, criterion, optimizer)\n","        test_loop(test_dl, model, criterion)\n"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["# Define the model\n","input_size = 187  # Number of input features\n","output_size = 5  # Output size (e.g., regression or binary classification)\n","model = SimpleANN(input_size, output_size)\n","\n","# Define loss and optimizer\n","criterion =  nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["train_ds = ECG_Dataset(df_train)\n","test_ds = ECG_Dataset(df_test)\n","train_dl, test_dl = get_data(train_ds, test_ds, 64)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1   -------------------------------\n","Train loss: 4.108393383503426\n","Test set => Accuracy: 95.4%, Avg loss: 1.103780 \n","\n","Epoch 2   -------------------------------\n","Train loss: 3.7743748459615745\n","Test set => Accuracy: 95.5%, Avg loss: 1.017283 \n","\n","Epoch 3   -------------------------------\n","Train loss: 3.628010864049429\n","Test set => Accuracy: 95.9%, Avg loss: 0.894511 \n","\n","Epoch 4   -------------------------------\n","Train loss: 3.2846517644939013\n","Test set => Accuracy: 96.0%, Avg loss: 0.803386 \n","\n","Epoch 5   -------------------------------\n","Train loss: 3.0611385184165556\n","Test set => Accuracy: 96.3%, Avg loss: 0.772160 \n","\n"]}],"source":["fit(5, model, criterion, optimizer, train_dl, test_dl)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":29414,"sourceId":37484,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
